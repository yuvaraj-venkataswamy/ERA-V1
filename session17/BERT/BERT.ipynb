{"cells":[{"cell_type":"code","source":["# mounting google drive folder\n","from google.colab import drive\n","drive.mount('/content/drive')\n","BASE_DIR='/content/drive/My Drive/S17/S17/BERT'\n","%cd $BASE_DIR"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"h7NQ7oP98QIM","executionInfo":{"status":"ok","timestamp":1696343450757,"user_tz":-330,"elapsed":4506,"user":{"displayName":"Atharv Yuvaraj","userId":"06637266434318105218"}},"outputId":"052949be-b97c-4fc9-eda7-fa80efc15108"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","/content/drive/My Drive/S17/S17/BERT\n"]}]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":512},"id":"pS50KH88749b","executionInfo":{"status":"error","timestamp":1696345845031,"user_tz":-330,"elapsed":2304,"user":{"displayName":"Atharv Yuvaraj","userId":"06637266434318105218"}},"outputId":"579f2de0-cf3e-4f90-f63a-bdd96acb3509"},"outputs":[{"output_type":"stream","name":"stdout","text":["initializing..\n","loading text...\n","tokenizing sentences...\n","creating/loading vocab...\n","creating dataset...\n","initializing model...\n"]},{"output_type":"error","ename":"RuntimeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-6-1acf3c2d6995>\u001b[0m in \u001b[0;36m<cell line: 261>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    259\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'initializing model...'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTransformer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_code\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_heads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membed_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minner_ff_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdropout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 261\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    262\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m \u001b[0;31m# =============================================================================\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mcuda\u001b[0;34m(self, device)\u001b[0m\n\u001b[1;32m    903\u001b[0m             \u001b[0mModule\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    904\u001b[0m         \"\"\"\n\u001b[0;32m--> 905\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    906\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    907\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mipu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    795\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    796\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 797\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    798\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    799\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    818\u001b[0m             \u001b[0;31m# `with torch.no_grad():`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    819\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 820\u001b[0;31m                 \u001b[0mparam_applied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    821\u001b[0m             \u001b[0mshould_use_set_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    822\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mshould_use_set_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    903\u001b[0m             \u001b[0mModule\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    904\u001b[0m         \"\"\"\n\u001b[0;32m--> 905\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    906\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    907\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mipu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/cuda/__init__.py\u001b[0m in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    245\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m'CUDA_MODULE_LOADING'\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m             \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'CUDA_MODULE_LOADING'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'LAZY'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 247\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cuda_init\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    248\u001b[0m         \u001b[0;31m# Some of the queued calls may reentrantly call _lazy_init();\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;31m# we need to just return without initializing in that case.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx"]}],"source":["# =============================================================================\n","# Libs\n","# =============================================================================\n","from torch.utils.data import Dataset\n","import torch.nn.functional as F\n","from collections import Counter\n","from os.path import exists\n","import torch.optim as optim\n","import torch.nn as nn\n","import numpy as np\n","import random\n","import torch\n","import math\n","import re\n","\n","\n","# =============================================================================\n","# Transformer\n","# =============================================================================\n","def attention(q, k, v, mask = None, dropout = None):\n","    scores = q.matmul(k.transpose(-2, -1))\n","    scores /= math.sqrt(q.shape[-1])\n","\n","    #mask\n","    scores = scores if mask is None else scores.masked_fill(mask == 0, -1e3)\n","\n","    scores = F.softmax(scores, dim = -1)\n","    scores = dropout(scores) if dropout is not None else scores\n","    output = scores.matmul(v)\n","    return output\n","\n","class MultiHeadAttention(nn.Module):\n","    def __init__(self, n_heads, out_dim, dropout=0.1):\n","        super().__init__()\n","\n","#        self.q_linear = nn.Linear(out_dim, out_dim)\n","#        self.k_linear = nn.Linear(out_dim, out_dim)\n","#        self.v_linear = nn.Linear(out_dim, out_dim)\n","        self.linear = nn.Linear(out_dim, out_dim*3)\n","\n","        self.n_heads = n_heads\n","        self.out_dim = out_dim\n","        self.out_dim_per_head = out_dim // n_heads\n","        self.out = nn.Linear(out_dim, out_dim)\n","        self.dropout = nn.Dropout(dropout)\n","\n","    def split_heads(self, t):\n","        return t.reshape(t.shape[0], -1, self.n_heads, self.out_dim_per_head)\n","\n","    def forward(self, x, y=None, mask=None):\n","        #in decoder, y comes from encoder. In encoder, y=x\n","        y = x if y is None else y\n","\n","        qkv = self.linear(x) # BS * SEQ_LEN * (3*EMBED_SIZE_L)\n","        q = qkv[:, :, :self.out_dim] # BS * SEQ_LEN * EMBED_SIZE_L\n","        k = qkv[:, :, self.out_dim:self.out_dim*2] # BS * SEQ_LEN * EMBED_SIZE_L\n","        v = qkv[:, :, self.out_dim*2:] # BS * SEQ_LEN * EMBED_SIZE_L\n","\n","        #break into n_heads\n","        q, k, v = [self.split_heads(t) for t in (q,k,v)]  # BS * SEQ_LEN * HEAD * EMBED_SIZE_P_HEAD\n","        q, k, v = [t.transpose(1,2) for t in (q,k,v)]  # BS * HEAD * SEQ_LEN * EMBED_SIZE_P_HEAD\n","\n","        #n_heads => attention => merge the heads => mix information\n","        scores = attention(q, k, v, mask, self.dropout) # BS * HEAD * SEQ_LEN * EMBED_SIZE_P_HEAD\n","        scores = scores.transpose(1,2).contiguous().view(scores.shape[0], -1, self.out_dim) # BS * SEQ_LEN * EMBED_SIZE_L\n","        out = self.out(scores)  # BS * SEQ_LEN * EMBED_SIZE\n","\n","        return out\n","\n","class FeedForward(nn.Module):\n","    def __init__(self, inp_dim, inner_dim, dropout=0.1):\n","        super().__init__()\n","        self.linear1 = nn.Linear(inp_dim, inner_dim)\n","        self.linear2 = nn.Linear(inner_dim, inp_dim)\n","        self.dropout = nn.Dropout(dropout)\n","\n","    def forward(self, x):\n","        #inp => inner => relu => dropout => inner => inp\n","        return self.linear2(self.dropout(F.relu(self.linear1(x))))\n","\n","class EncoderLayer(nn.Module):\n","    def __init__(self, n_heads, inner_transformer_size, inner_ff_size, dropout=0.1):\n","        super().__init__()\n","        self.mha = MultiHeadAttention(n_heads, inner_transformer_size, dropout)\n","        self.ff = FeedForward(inner_transformer_size, inner_ff_size, dropout)\n","        self.norm1 = nn.LayerNorm(inner_transformer_size)\n","        self.norm2 = nn.LayerNorm(inner_transformer_size)\n","        self.dropout1 = nn.Dropout(dropout)\n","        self.dropout2 = nn.Dropout(dropout)\n","\n","    def forward(self, x, mask=None):\n","        x2 = self.norm1(x)\n","        x = x + self.dropout1(self.mha(x2, mask=mask))\n","        x2 = self.norm2(x)\n","        x = x + self.dropout2(self.ff(x2))\n","        return x\n","\n","class Transformer(nn.Module):\n","    def __init__(self, n_code, n_heads, embed_size, inner_ff_size, n_embeddings, seq_len, dropout=.1):\n","        super().__init__()\n","\n","        #model input\n","        self.embeddings = nn.Embedding(n_embeddings, embed_size)\n","        self.pe = PositionalEmbedding(embed_size, seq_len)\n","\n","        #backbone\n","        encoders = []\n","        for i in range(n_code):\n","            encoders += [EncoderLayer(n_heads, embed_size, inner_ff_size, dropout)]\n","        self.encoders = nn.ModuleList(encoders)\n","\n","        #language model\n","        self.norm = nn.LayerNorm(embed_size)\n","        self.linear = nn.Linear(embed_size, n_embeddings, bias=False)\n","\n","\n","    def forward(self, x):\n","        x = self.embeddings(x)\n","        x = x + self.pe(x)\n","        for encoder in self.encoders:\n","            x = encoder(x)\n","        x = self.norm(x)\n","        x = self.linear(x)\n","        return x\n","\n","# Positional Embedding\n","class PositionalEmbedding(nn.Module):\n","    def __init__(self, d_model, max_seq_len = 80):\n","        super().__init__()\n","        self.d_model = d_model\n","        pe = torch.zeros(max_seq_len, d_model)\n","        pe.requires_grad = False\n","        for pos in range(max_seq_len):\n","            for i in range(0, d_model, 2):\n","                pe[pos, i] = math.sin(pos / (10000 ** ((2 * i)/d_model)))\n","                pe[pos, i + 1] = math.cos(pos / (10000 ** ((2 * (i + 1))/d_model)))\n","        pe = pe.unsqueeze(0)\n","        self.register_buffer('pe', pe)\n","\n","    def forward(self, x):\n","        return self.pe[:,:x.size(1)] #x.size(1) = seq_len\n","\n","# =============================================================================\n","# Dataset\n","# =============================================================================\n","class SentencesDataset(Dataset):\n","    #Init dataset\n","    def __init__(self, sentences, vocab, seq_len):\n","        dataset = self\n","\n","        dataset.sentences = sentences\n","        dataset.vocab = vocab + ['<ignore>', '<oov>', '<mask>']\n","        dataset.vocab = {e:i for i, e in enumerate(dataset.vocab)}\n","        dataset.rvocab = {v:k for k,v in dataset.vocab.items()}\n","        dataset.seq_len = seq_len\n","\n","        #special tags\n","        dataset.IGNORE_IDX = dataset.vocab['<ignore>'] #replacement tag for tokens to ignore\n","        dataset.OUT_OF_VOCAB_IDX = dataset.vocab['<oov>'] #replacement tag for unknown words\n","        dataset.MASK_IDX = dataset.vocab['<mask>'] #replacement tag for the masked word prediction task\n","\n","\n","    #fetch data\n","    def __getitem__(self, index, p_random_mask=0.15):\n","        dataset = self\n","\n","        #while we don't have enough word to fill the sentence for a batch\n","        s = []\n","        while len(s) < dataset.seq_len:\n","            s.extend(dataset.get_sentence_idx(index % len(dataset)))\n","            index += 1\n","\n","        #ensure that the sequence is of length seq_len\n","        s = s[:dataset.seq_len]\n","        [s.append(dataset.IGNORE_IDX) for i in range(dataset.seq_len - len(s))] #PAD ok\n","\n","        #apply random mask\n","        s = [(dataset.MASK_IDX, w) if random.random() < p_random_mask else (w, dataset.IGNORE_IDX) for w in s]\n","\n","        return {'input': torch.Tensor([w[0] for w in s]).long(),\n","                'target': torch.Tensor([w[1] for w in s]).long()}\n","\n","    #return length\n","    def __len__(self):\n","        return len(self.sentences)\n","\n","    #get words id\n","    def get_sentence_idx(self, index):\n","        dataset = self\n","        s = dataset.sentences[index]\n","        s = [dataset.vocab[w] if w in dataset.vocab else dataset.OUT_OF_VOCAB_IDX for w in s]\n","        return s\n","\n","# =============================================================================\n","# Methods / Class\n","# =============================================================================\n","def get_batch(loader, loader_iter):\n","    try:\n","        batch = next(loader_iter)\n","    except StopIteration:\n","        loader_iter = iter(loader)\n","        batch = next(loader_iter)\n","    return batch, loader_iter\n","\n","# =============================================================================\n","# #Init\n","# =============================================================================\n","print('initializing..')\n","batch_size = 1024\n","seq_len = 20\n","embed_size = 128\n","inner_ff_size = embed_size * 4\n","n_heads = 8\n","n_code = 8\n","n_vocab = 40000\n","dropout = 0.1\n","# n_workers = 12\n","\n","#optimizer\n","optim_kwargs = {'lr':1e-4, 'weight_decay':1e-4, 'betas':(.9,.999)}\n","\n","# =============================================================================\n","# Input\n","# =============================================================================\n","#1) load text\n","print('loading text...')\n","pth = 'training.txt'\n","sentences = open(pth).read().lower().split('\\n')\n","\n","#2) tokenize sentences (can be done during training, you can also use spacy udpipe)\n","print('tokenizing sentences...')\n","special_chars = ',?;.:/*!+-()[]{}\"\\'&'\n","sentences = [re.sub(f'[{re.escape(special_chars)}]', ' \\g<0> ', s).split(' ') for s in sentences]\n","sentences = [[w for w in s if len(w)] for s in sentences]\n","\n","#3) create vocab if not already created\n","print('creating/loading vocab...')\n","pth = 'vocab.txt'\n","if not exists(pth):\n","    words = [w for s in sentences for w in s]\n","    vocab = Counter(words).most_common(n_vocab) #keep the N most frequent words\n","    vocab = [w[0] for w in vocab]\n","    open(pth, 'w+').write('\\n'.join(vocab))\n","else:\n","    vocab = open(pth).read().split('\\n')\n","\n","#4) create dataset\n","print('creating dataset...')\n","dataset = SentencesDataset(sentences, vocab, seq_len)\n","# kwargs = {'num_workers':n_workers, 'shuffle':True,  'drop_last':True, 'pin_memory':True, 'batch_size':batch_size}\n","kwargs = {'shuffle':True,  'drop_last':True, 'pin_memory':True, 'batch_size':batch_size}\n","data_loader = torch.utils.data.DataLoader(dataset, **kwargs)\n","\n","\n","# =============================================================================\n","# Model\n","# =============================================================================\n","#init model\n","print('initializing model...')\n","model = Transformer(n_code, n_heads, embed_size, inner_ff_size, len(dataset.vocab), seq_len, dropout)\n","model = model.cuda()\n","\n","# =============================================================================\n","# Optimizer\n","# =============================================================================\n","print('initializing optimizer and loss...')\n","optimizer = optim.Adam(model.parameters(), **optim_kwargs)\n","loss_model = nn.CrossEntropyLoss(ignore_index=dataset.IGNORE_IDX)\n","\n","# =============================================================================\n","# Train\n","# =============================================================================\n","print('training...')\n","print_each = 10\n","model.train()\n","batch_iter = iter(data_loader)\n","n_iteration = 10000\n","for it in range(n_iteration):\n","\n","    #get batch\n","    batch, batch_iter = get_batch(data_loader, batch_iter)\n","\n","    #infer\n","    masked_input = batch['input']\n","    masked_target = batch['target']\n","\n","    masked_input = masked_input.cuda(non_blocking=True)\n","    masked_target = masked_target.cuda(non_blocking=True)\n","    output = model(masked_input)\n","\n","    #compute the cross entropy loss\n","    output_v = output.view(-1,output.shape[-1])\n","    target_v = masked_target.view(-1,1).squeeze()\n","    loss = loss_model(output_v, target_v)\n","\n","    #compute gradients\n","    loss.backward()\n","\n","    #apply gradients\n","    optimizer.step()\n","\n","    #print step\n","    if it % print_each == 0:\n","        print('it:', it,\n","              ' | loss', np.round(loss.item(),2),\n","              ' | Δw:', round(model.embeddings.weight.grad.abs().sum().item(),3))\n","\n","    #reset gradients\n","    optimizer.zero_grad()\n","\n","\n","# =============================================================================\n","# Results analysis\n","# =============================================================================\n","print('saving embeddings...')\n","N = 3000\n","np.savetxt('values.tsv', np.round(model.embeddings.weight.detach().cpu().numpy()[0:N], 2), delimiter='\\t', fmt='%1.2f')\n","s = [dataset.rvocab[i] for i in range(N)]\n","open('names.tsv', 'w+').write('\\n'.join(s) )\n","\n","\n","print('end')\n","\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6FauqovO749g"},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"base","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.7"},"orig_nbformat":4,"vscode":{"interpreter":{"hash":"b8fbfcbe0e544000e4ba3d2d9974592a7ba1a2af52205db5302ae41a0c45d995"}},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}